{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMABrF853vpu7Kh8N54ETfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narae00/DeepLearning/blob/main/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3A4GpGXVcJNf"
      },
      "outputs": [],
      "source": [
        "#AutoEncoder.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class AutoEncoder:\n",
        "  def __init__(self):\n",
        "    self.encoder= None\n",
        "    self.decoder= None\n",
        "    self.en_decoder= None\n",
        "    self.relu= tf.keras.activations.relu\n",
        "    self.tanh= tf.keras.activations.tanh\n",
        "    self.input_output_dim= 784\n",
        "    self.encoder_hidden_layers= [200, 200]\n",
        "    self.decoder_hidden_layers= [200, 200]\n",
        "    self.code_dim= 32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(self):\n",
        "  # Build Encoder\n",
        "  encoder_input= tf.keras.layers.Input(shape=(self.input_output_dim, ), dtype=tf.float32)\n",
        "  encoder_h_layer= encoder_input\n",
        "  for dim in self.encoder_hidden_layers:\n",
        "    encoder_h_layer= tf.keras.layers.Dense(\n",
        "        units=dim, activation=self.relu, use_bias=True\n",
        "        )(encoder_h_layer)\n",
        "  code = tf.keras.layers.Dense(\n",
        "      units=self.code_dim, activation=self.tanh, use_bias=True\n",
        "      )(encoder_h_layer)\n",
        "  self.encoder= tf.keras.models.Model(inputs=encoder_input, outputs=code)\n",
        "  # Build Decoder\n",
        "  decoder_input= tf.keras.layers.Input(shape=(self.code_dim, ), dtype=tf.float32)\n",
        "  decoder_h_layer= decoder_input\n",
        "  for dim in self.decoder_hidden_layers:\n",
        "    decoder_h_layer= tf.keras.layers.Dense(\n",
        "        units=dim, activation=self.relu, use_bias=True\n",
        "        )(decoder_h_layer)\n",
        "\n",
        "  decoder_output= tf.keras.layers.Dense(\n",
        "    units=self.input_output_dim, activation=None, use_bias=True\n",
        "  )(decoder_h_layer)\n",
        "  self.decoder= tf.keras.models.Model(inputs=decoder_input, outputs=decoder_output)\n",
        "\n",
        "  # En-Decoder\n",
        "  vae_output= self.decoder(code)\n",
        "  self.en_decoder= tf.keras.models.Model(inputs=encoder_input, outputs=vae_output)\n",
        "  optimizer_alg= tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  mse= tf.keras.losses.mse\n",
        "  self.en_decoder.compile(optimizer=optimizer_alg, loss=mse)\n",
        "\n",
        "def fit(self, x, y, batch_size, epochs):\n",
        "  self.en_decoder.fit(x=x, y=y, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "def save_weights(self, save_path):\n",
        "  self.en_decoder.save_weights(save_path)\n",
        "\n",
        "def load_weights(self, load_path):\n",
        "  self.en_decoder.load_weights(load_path)"
      ],
      "metadata": {
        "id": "NPmAtCSzcqGp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main.py\n",
        "from MNISTData import MNISTData\n",
        "from AutoEncoder import AutoEncoder\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  print(\"Hi. I am an Auto Encoder Trainer.\")\n",
        "  batch_size = 32\n",
        "  num_epochs = 5\n",
        "\n",
        "  data_loader = MNISTData()\n",
        "  data_loader.load_data()\n",
        "\n",
        "  x_train = data_loader.x_train\n",
        "  input_output_dim = data_loader.in_out_dim\n",
        "  auto_encoder = AutoEncoder()\n",
        "  auto_encoder.build_model()\n",
        "  auto_encoder.fit(x=x_train, y=x_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "  save_path = \"./models/ae_model.weights.h5\"\n",
        "  auto_encoder.save_model(save_path)\n",
        "  print(\"load model weights from %s\" %save_path)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "5o0M_4yXePHW",
        "outputId": "e904c6a1-55b0-4c9c-fb30-28d7ee611bae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'MNISTData'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fe819cb268d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#main.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mMNISTData\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMNISTData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mAutoEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MNISTData'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#main_test.py\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from MNISTData import MNISTData\n",
        "from AutoEncoder import AutoEncoder\n",
        "import numpyas np\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(\"Hi. I am an AutoEncoderTester.\")\n",
        "  batch_size= 32\n",
        "  num_epochs= 5\n",
        "  data_loader= MNISTData()\n",
        "  data_loader.load_data()\n",
        "  x_train= data_loader.x_train\n",
        "  input_output_dim= data_loader.in_out_dim\n",
        "\n",
        "  auto_encoder= AutoEncoder()\n",
        "  auto_encoder.build_model()\n",
        "  load_path= \"./model/ae_model.weights.h5\"\n",
        "  print(\"load model weights from %s\" % load_path)\n",
        "  auto_encoder.load_weights(load_path)\n",
        "  # print for test\n",
        "  num_test_items= 56\n",
        "  test_data= data_loader.x_test[0:num_test_items, :]\n",
        "  test_label= data_loader.y_test[0:num_test_items]\n",
        "  test_data_x_print= test_data.reshape(num_test_items, data_loader.width, data_loader.height)\n",
        "\n",
        "  print(\"const by codes\")\n",
        "  reconst_data= auto_encoder.en_decoder.predict(test_data)\n",
        "  reconst_data_x_print= reconst_data.reshape(num_test_items, data_loader.width, data_loader.height)\n",
        "  reconst_data_x_print= tf.math.sigmoid(reconst_data_x_print)\n",
        "  MNISTData.print_56_pair_images(test_data_x_print, reconst_data_x_print, test_label)\n",
        "  print(\"const by code means for each digit\")\n",
        "  avg_codes= np.zeros([10, 32])\n",
        "  avg_add_cnt= np.zeros([10])\n",
        "\n",
        "  latent_vecs= auto_encoder.encoder.predict(test_data)\n",
        "  for i, label in enumerate(test_label):\n",
        "    avg_codes[label] = latent_vecs[i]\n",
        "    avg_add_cnt[label] += 1.0\n",
        "    for i in range(10):\n",
        "      if avg_add_cnt[label] > 0.1:\n",
        "        avg_codes[i] /= avg_add_cnt[label]\n",
        "\n",
        "  avg_code_tensor= tf.convert_to_tensor(avg_codes)\n",
        "  reconst_data_by_vecs= auto_encoder.decoder.predict(avg_code_tensor)\n",
        "  reconst_data_x_by_mean_print= reconst_data_by_vecs.reshape(10, data_loader.width, data_loader.height)\n",
        "  label_list= [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "  MNISTData.print_10_images(reconst_data_x_by_mean_print, label_list)"
      ],
      "metadata": {
        "id": "tspsdoK0fn0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}